{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1jkEmNWlM2Hc"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iw5H76hpM6LX"
   },
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aR5nouI2M8Sz",
    "outputId": "b94b78c9-290c-44a6-f398-61fcc1e8da3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YcEYUKE-iNGl",
    "outputId": "3acc91c0-c38e-4ab0-a41e-be725c9df20d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FHby_EflM-fX"
   },
   "outputs": [],
   "source": [
    "text = text[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VSsxuTJrNB0o"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from nltk.tokenize import TweetTokenizer # doesn't split at apostrophes\n",
    "import nltk\n",
    "from nltk import Text\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from gensim import corpora\n",
    "from gensim.models import Phrases\n",
    "# from ds_voc.text_processing import TextProcessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9w2_AKoND9A"
   },
   "outputs": [],
   "source": [
    "cleaned = []\n",
    "def cleaning(text):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "#     convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "#     remove punctuation from each word\n",
    "    import string\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "#     remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in stripped if not w in stop_words]\n",
    "    # lemmatizing of words\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatize = ' '.join(lemmatizer.lemmatize(word) for word in words)\n",
    "    return lemmatize\n",
    "#     cleaned.append(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "VN3J6F5oNO1d",
    "outputId": "70d01ff8-a585-454a-87ea-439954b4dbd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xHv9a88lNGtU"
   },
   "outputs": [],
   "source": [
    "cleaned = [cleaning(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TGUhngNy23w0",
    "outputId": "b0638493-90e0-4b56-8e09-ff971a6eaeda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31280"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "19cBgVQ0NJEz",
    "outputId": "61781156-6fe8-4b6b-bbe1-3ed0fb406cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result embedding shape: (1751, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word_model = Word2Vec(sentences = [cleaned[0].split()], size=100, min_count=1, \n",
    "                                    window=5, iter=100)\n",
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "# print('Checking similar words:')\n",
    "# for word in ['model', 'network', 'train', 'learn']:\n",
    "#   most_similar = ', '.join('%s (%.2f)' % (similar, dist) \n",
    "#                            for similar, dist in word_model.most_similar(word)[:8])\n",
    "#   print('  %s -> %s' % (word, most_similar))\n",
    "\n",
    "def word2idx(word):\n",
    "  return word_model.wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "  return word_model.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YTVtzWnz3KGz",
    "outputId": "a802acd8-fd21-4606-e4c9-0623530fb132"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1751, 100)"
      ]
     },
     "execution_count": 88,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2g4x-cuOhXi"
   },
   "outputs": [],
   "source": [
    "data = cleaned[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IHdiIy_T3hKj",
    "outputId": "79e9b862-d793-4954-d1d7-2db320e2d55c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4519"
      ]
     },
     "execution_count": 90,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wyNd28K6N9JN"
   },
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(data) - seq_length, 1):\n",
    "    dataX.append(data[i:i + seq_length])\n",
    "    dataY.append(data[i + seq_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "spIOrO0PPBCv"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UhcCREKaRQDr",
    "outputId": "b8df6f08-2ce5-4cf7-b9d9-137ee7fde6e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4419"
      ]
     },
     "execution_count": 92,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEX3T8VuOvwC"
   },
   "outputs": [],
   "source": [
    "train_x = np.zeros([len(dataX), seq_length], dtype=np.int32)\n",
    "train_y = np.zeros([len(dataX)], dtype=np.int32)\n",
    "for i, sentence in enumerate(dataX):\n",
    "  for t, word in enumerate(sentence):\n",
    "    train_x[i, t] = word2idx(word)\n",
    "  train_y[i] = word2idx(dataY[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2V0JQFT4fjVX"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1w4mHY_fwdP"
   },
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TZ7Lg6d7VwQr",
    "outputId": "6641e135-9f12-4813-caa2-e83e2da01d07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 18s - loss: 7.2618 - acc: 0.0154\n",
      "Epoch 2/50\n",
      " - 16s - loss: 6.8785 - acc: 0.0199\n",
      "Epoch 3/50\n",
      " - 16s - loss: 6.7417 - acc: 0.0226\n",
      "Epoch 4/50\n",
      " - 16s - loss: 6.6530 - acc: 0.0260\n",
      "Epoch 5/50\n",
      " - 16s - loss: 6.4542 - acc: 0.0310\n",
      "Epoch 6/50\n",
      " - 16s - loss: 6.2463 - acc: 0.0333\n",
      "Epoch 7/50\n",
      " - 16s - loss: 6.0346 - acc: 0.0398\n",
      "Epoch 8/50\n",
      " - 16s - loss: 5.8307 - acc: 0.0412\n",
      "Epoch 9/50\n",
      " - 16s - loss: 5.6269 - acc: 0.0505\n",
      "Epoch 10/50\n",
      " - 16s - loss: 5.4381 - acc: 0.0561\n",
      "Epoch 11/50\n",
      " - 16s - loss: 5.2452 - acc: 0.0695\n",
      "Epoch 12/50\n",
      " - 16s - loss: 5.0650 - acc: 0.0783\n",
      "Epoch 13/50\n",
      " - 16s - loss: 4.8774 - acc: 0.0935\n",
      "Epoch 14/50\n",
      " - 16s - loss: 4.6971 - acc: 0.1143\n",
      "Epoch 15/50\n",
      " - 16s - loss: 4.5173 - acc: 0.1428\n",
      "Epoch 16/50\n",
      " - 16s - loss: 4.3377 - acc: 0.1774\n",
      "Epoch 17/50\n",
      " - 16s - loss: 4.1587 - acc: 0.2084\n",
      "Epoch 18/50\n",
      " - 16s - loss: 3.9830 - acc: 0.2548\n",
      "Epoch 19/50\n",
      " - 16s - loss: 3.8046 - acc: 0.3039\n",
      "Epoch 20/50\n",
      " - 16s - loss: 3.6305 - acc: 0.3394\n",
      "Epoch 21/50\n",
      " - 16s - loss: 3.4567 - acc: 0.3777\n",
      "Epoch 22/50\n",
      " - 16s - loss: 3.2927 - acc: 0.4164\n",
      "Epoch 23/50\n",
      " - 16s - loss: 3.1228 - acc: 0.4553\n",
      "Epoch 24/50\n",
      " - 16s - loss: 2.9637 - acc: 0.4945\n",
      "Epoch 25/50\n",
      " - 16s - loss: 2.8079 - acc: 0.5200\n",
      "Epoch 26/50\n",
      " - 16s - loss: 2.6546 - acc: 0.5467\n",
      "Epoch 27/50\n",
      " - 16s - loss: 2.5171 - acc: 0.5700\n",
      "Epoch 28/50\n",
      " - 16s - loss: 2.3753 - acc: 0.5997\n",
      "Epoch 29/50\n",
      " - 16s - loss: 2.2432 - acc: 0.6241\n",
      "Epoch 30/50\n",
      " - 16s - loss: 2.1255 - acc: 0.6479\n",
      "Epoch 31/50\n",
      " - 16s - loss: 2.0031 - acc: 0.6664\n",
      "Epoch 32/50\n",
      " - 16s - loss: 1.8898 - acc: 0.6884\n",
      "Epoch 33/50\n",
      " - 16s - loss: 1.7881 - acc: 0.7045\n",
      "Epoch 34/50\n",
      " - 16s - loss: 1.6885 - acc: 0.7248\n",
      "Epoch 35/50\n",
      " - 16s - loss: 1.6064 - acc: 0.7391\n",
      "Epoch 36/50\n",
      " - 16s - loss: 1.5169 - acc: 0.7508\n",
      "Epoch 37/50\n",
      " - 16s - loss: 1.4351 - acc: 0.7701\n",
      "Epoch 38/50\n",
      " - 16s - loss: 1.3472 - acc: 0.7875\n",
      "Epoch 39/50\n",
      " - 16s - loss: 1.2751 - acc: 0.7995\n",
      "Epoch 40/50\n",
      " - 16s - loss: 1.2052 - acc: 0.8144\n",
      "Epoch 41/50\n",
      " - 16s - loss: 1.1356 - acc: 0.8312\n",
      "Epoch 42/50\n",
      " - 16s - loss: 1.0747 - acc: 0.8389\n",
      "Epoch 43/50\n",
      " - 16s - loss: 1.0158 - acc: 0.8527\n",
      "Epoch 44/50\n",
      " - 16s - loss: 0.9571 - acc: 0.8638\n",
      "Epoch 45/50\n",
      " - 16s - loss: 0.9059 - acc: 0.8753\n",
      "Epoch 46/50\n",
      " - 16s - loss: 0.8511 - acc: 0.8859\n",
      "Epoch 47/50\n",
      " - 16s - loss: 0.8028 - acc: 0.8927\n",
      "Epoch 48/50\n",
      " - 16s - loss: 0.7626 - acc: 0.9004\n",
      "Epoch 49/50\n",
      " - 16s - loss: 0.7143 - acc: 0.9090\n",
      "Epoch 50/50\n",
      " - 16s - loss: 0.6753 - acc: 0.9149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f69cac468d0>"
      ]
     },
     "execution_count": 167,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "# integer encode the documents\n",
    "# encoded_docs= [one_hot(d[0], vocab_size) for d in enc]\n",
    "# encoded_docs_y= [one_hot(d, vocab_size) for d in dataY]\n",
    "\n",
    "# print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 100\n",
    "# padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# print(padded_docs)\n",
    "# define the model\n",
    "models = Sequential()\n",
    "models.add(Embedding(vocab_size, emdedding_size,weights=[pretrained_weights]))\n",
    "models.add(LSTM(units=emdedding_size))\n",
    "models.add(Dense(units=vocab_size))\n",
    "models.add(Activation('softmax'))\n",
    "models.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "# summarize the model\n",
    "# print(model.summary())\n",
    "# fit the model\n",
    "models.fit(train_x, train_y, epochs=50, verbose=2)\n",
    "# # evaluate the model\n",
    "# loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "# print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IHHCqqWeDH8N",
    "outputId": "0fc271c6-dbfe-48dc-db47-ec4004b89c0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 134,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtXVkhXxD1fy"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "  if temperature <= 0:\n",
    "    return np.argmax(preds)\n",
    "  preds = np.asarray(preds).astype('float64')\n",
    "  preds = np.log(preds) / temperature\n",
    "  exp_preds = np.exp(preds)\n",
    "  preds = exp_preds / np.sum(exp_preds)\n",
    "  probas = np.random.multinomial(1, preds, 1)\n",
    "  return np.argmax(probas)\n",
    "\n",
    "def generate_next(text, num_generated=100):\n",
    "#   print(text)\n",
    "  word_idxs = [word2idx(word) for word in text]\n",
    "  print(np.array(word_idxs).shape)\n",
    "  for i in range(num_generated):\n",
    "    prediction = models.predict(x=np.array(word_idxs))\n",
    "    idx = sample(prediction[-1], temperature=0.7)\n",
    "    word_idxs.append(idx)\n",
    "  return ' '.join(idx2word(idx) for idx in word_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "mXyJB8qZFoYO",
    "outputId": "f442e5cd-0e0b-4d17-8948-5288f7d6612b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'speak speak speak first citizen resolved rather die famish resolved resolved first citizen first know caius marcius chief enemy people knowt knowt first citizen let u kill corn price ist verdict talking nt let done away away second citizen one word good citizen first citizen accounted poor citizen patrician good authority surfeit would relieve u would yield u superfluity wholesome might guess relieved u humanely think dear leanness afflicts u object misery inventory particularise abundance sufferance gain let u revenge pike ere become rake god know speak hunger bread thirst revenge second citizen would proceed especially caius marcius first dog answer first citizen answer strike hath part wall shall action none wish wish crack devise silenced mummer doit womb nicelygawded ridge affection tenth butterfly safe fair volumnia last state fashion hark brother part dog miscarries bear menenius word volumnia good report senator come opinion coffin renown thousand beastly bear come messenger three three rome gentle rome town live turn god welcome noble pray cominius live return prithee lartius horse business sicinius maliciously service love general herald leg dispatch name deedachieving bestow spun unactive provide risen prevail whereof remember carry caius begin fare forth fall husewife ay menenius well menenius fob'"
      ]
     },
     "execution_count": 203,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_next(np.array(dataX[4]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "FInal.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
